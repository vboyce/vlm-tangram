---
title: "First pass"
output:
  html_document:
    df_print: paged
---


# TODO Veronica

* look at per-tangram codeability metrics from kilogram -- does this track human and/or model accuracies
* look at mpt-accuracies at per-tangram -- how do these track with naive-human and/or model accuracies

# Thoughts for later

* ? try more comparable to tg-matcher in presenting full-ish transcripts?
* should we retrain on not these tangrams, only others? (is there a pre-trained model that achieves this?)
* ? should we split the utterances somehow and look at fit of words/phrases (i.e. to feed to CHAI? or do drop out analysis or ....)


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(viridis)
library(Replicate)
library(metafor)
library(esc)
library(here)
library(brms)
library(rstan)
library(googledrive)
library(glmnet)
library(tidybayes)
library(ggstance)
library("lattice")
library(reshape2)
library(ggrepel)
library(ggthemes)
library(knitr)
library(cowplot)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(ggtext)

theme_set(theme_bw())

images <- "images"
```

```{r}
# let's try reading them in from github? since they're in another repo

# based on https://github.com/vboyce/multiparty-tangrams/blob/main/code/prep_ms.R

url <- "https://raw.githubusercontent.com/vboyce/multiparty-tangrams/main/"
one_chat <- read_csv(str_c(url, "data/study1/filtered_chat.csv")) |> mutate(rotate = str_c(as.character(numPlayers), "_rotate"))
two_a_chat <- read_csv(str_c(url, "data/study2a/filtered_chat.csv")) |> mutate(rotate = "no_rotate")
two_b_chat <- read_csv(str_c(url, "data/study2b/filtered_chat.csv")) |>
  mutate(rotate = "full_feedback") |>
  select(-`row num`)
two_c_chat <- read_csv(str_c(url, "data/study2c/filtered_chat.csv")) |>
  mutate(rotate = "emoji") |>
  select(-type)
three_chat <- read_csv(str_c(url, "data/study3/filtered_chat.csv")) |>
  inner_join(read_rds(str_c(url, "data/study3/round_results.rds")) |> select(gameId, trialNum, condition = name) |> unique()) |>
  select(-rowid, -type)

combined_chat <- one_chat |>
  rbind(two_a_chat) |>
  rbind(two_b_chat) |>
  rbind(two_c_chat) |>
  mutate(activePlayerCount = NA) |>
  rename(condition = rotate) |>
  rbind(three_chat) |>
  filter(!(is.chitchat)) |>
  mutate(
    text = gsub("\\n", "", fixed = T, spellchecked), # note that this is using spellcorrected version!!!!
    text = gsub("[/?/.]", " ", text),
    text = str_squish(text),
    tangram = gsub("/experiment/tangram_", "", target, fixed = TRUE),
    tangram = gsub(".png", "", tangram, fixed = TRUE)
  ) %>%
  select(gameId, trialNum, repNum, tangram, playerId, role, numPlayers, text, condition)
# so here we instead count non-white space chunks for words
```


```{r}
d <- read_csv(here("clip_stimulus_level_predictions.csv")) |> left_join(combined_chat)
```

# Analyses just of this

How often is the highest likelihood label the correct one?

```{r}
color_scheme <- c(
  "2_rotate" = "#FFBDD4", "5_rotate" = "#A12EFF", "3_rotate" = "#FF7DF0", "6_rotate" = "#6940FF", "4_rotate" = "#D24AFF", "full_feedback" = "#425df5", "no_rotate" = "#00A2FF", "emoji" = "#D47E04", "2_thin" = "#FFDA09", "6_thin" = "#D47E04",
  "2_thick" = "#77F3DB", "6_thick" = "#00BDA8"
)

d |>
  mutate(correct = prediction == label) |>
  mutate(expt = case_when(
    condition %in% c("emoji", "full_feedback", "no_rotate") ~ 2,
    str_detect(condition, "rotate") ~ 1,
    T ~ 3
  )) |>
  group_by(condition, repNum, expt) |>
  summarize(model_correct = mean(correct)) |>
  ggplot(aes(x = repNum, y = model_correct, color = condition)) +
  geom_point() +
  geom_line() +
  facet_wrap(~expt) +
  scale_color_manual(values = color_scheme) +
  coord_cartesian(ylim = c(0, 1), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dotted")
```

Split by tangram. 
We know that tangrams vary in codeability. 

```{r}
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

label <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")



by_tangram <- d |>
  mutate(correct = prediction == label) |>
  group_by(label, repNum) |>
  summarize(model_correct = mean(correct))

foo <- tibble(label, labels) |>
  left_join(by_tangram) |>
  group_by(label, labels) |>
  summarize(m = mean(model_correct)) |>
  arrange(m)

by_tangram |> ggplot(aes(x = reorder(label, model_correct), y = model_correct, color = as.factor(repNum))) +
  geom_point() +
  scale_color_viridis(discrete = T) +
  coord_cartesian(ylim = c(0, 1)) +
  geom_hline(yintercept = 1 / 12, linetype = "dotted") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text.x = element_markdown(color = "black", size = 11))
```

## By probability assigned

Alternative is to look at how much probability the correct answer got.

```{r}
by_probability <- d |>
  pivot_longer(p_A:p_L, names_to = "image", values_to = "prob") |>
  filter(str_c("p_", label) == image)


by_probability |>
  mutate(correct = prediction == label) |>
  mutate(expt = case_when(
    condition %in% c("emoji", "full_feedback", "no_rotate") ~ 2,
    str_detect(condition, "rotate") ~ 1,
    T ~ 3
  )) |>
  ggplot(aes(x = repNum, y = prob, color = condition)) +
  stat_summary(fun.data = "mean_cl_boot") +
  stat_summary(fun.data = "mean_cl_boot", geom = "line") +
  facet_wrap(~expt) +
  scale_color_manual(values = color_scheme) +
  geom_hline(yintercept = 1 / 12, linetype = "dotted")


mean_prob <- by_probability |>
  group_by(label) |>
  summarize(m = mean(prob))

foo <- tibble(label, labels) |>
  left_join(mean_prob) |>
  arrange(m)

by_probability |> ggplot(aes(x = reorder(label, prob), y = prob, color = as.factor(repNum))) +
  stat_summary(aes(group = repNum), fun.data = "mean_cl_boot") +
  geom_hline(yintercept = 1 / 12, linetype = "dotted") +
  scale_color_viridis(discrete = T) +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text.x = element_markdown(color = "black", size = 11))
```


## confusion matrices

Of top option. 

```{r}
library(ggtext)
label <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

confusion <- d |>
  group_by(label, prediction) |>
  tally() |>
  group_by(label) |>
  mutate(pct = n / sum(n))

self <- confusion |>
  filter(label == prediction) |>
  select(label, self = pct)

corr_order <- tibble(label, labels) |>
  left_join(self) |>
  arrange(self)

self_2 <- confusion |>
  ungroup() |>
  filter(label == prediction) |>
  select(prediction, self_2 = pct)


confusion |>
  left_join(self) |>
  left_join(self_2) |>
  ggplot(aes(x = reorder(label, self, FUN = mean), y = reorder(prediction, self_2, FUN = mean), fill = pct)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  scale_x_discrete(name = "Correct", labels = corr_order$labels) +
  scale_y_discrete(name = "Model label", labels = corr_order$labels) +
  theme(axis.text = element_markdown(color = "black", size = 11))
```

Of probability mass. 

```{r}
library(ggtext)
label <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

confusion <- d |>
  pivot_longer(p_A:p_L, names_to = "image", values_to = "prob") |>
  mutate(prediction = str_sub(image, -1)) |>
  group_by(label, prediction) |>
  summarize(prob = mean(prob))

self <- confusion |>
  filter(label == prediction) |>
  select(label, self = prob)

corr_order <- tibble(label, labels) |>
  left_join(self) |>
  arrange(self)

self_2 <- confusion |>
  ungroup() |>
  filter(label == prediction) |>
  select(prediction, self_2 = prob)


confusion |>
  left_join(self) |>
  left_join(self_2) |>
  ggplot(aes(x = reorder(label, self, FUN = mean), y = reorder(prediction, self_2, FUN = mean), fill = prob)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  scale_x_discrete(name = "Correct", labels = corr_order$labels) +
  scale_y_discrete(name = "Model option", labels = corr_order$labels) +
  theme(axis.text = element_markdown(color = "black", size = 11))
```

# Compare with people 

Basically, we want to know how the model qualitatively compares to humans -- i.e. is there alignment on what the harder / easier ones are. 

Could look at this various ways, but the cleanest comparison is that we have naive human guessing data. 

```{r}
human <- read_csv("human_data.csv")

d_subset <- d |>
  filter(repNum %in% c(0, 5)) |>
  filter(condition %in% c("2_rotate", "6_rotate", "2_thin", "6_thin", "2_thick", "6_thick"))
```

```{r}
human_summary <- human |>
  group_by(tangram = correct_tangram, condition, round) |>
  summarize(correct = mean(correct)) |> 
  mutate(source="human")

model_summary <- d_subset |>
  mutate(round = str_c("round_",as.character(repNum + 1))) |>
  mutate(correct = tangram == prediction) |>
  group_by(tangram, condition, round) |>
  summarize(correct = mean(correct)) |> 
  mutate(source="model")

both <- human_summary |> bind_rows(model_summary) 

label <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

foo <- tibble(tangram=label,labels) |> left_join(both) |> 
  group_by(tangram, labels) |> 
  summarize(acc=mean(correct)) |> 
  arrange(acc)

ggplot(both, aes(x=reorder(tangram, correct), y=correct, group=source, color=source))+geom_point(position=position_dodge(width=.3))+
    scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text = element_markdown(color = "black", size = 11))
```

Each point is one of the 12 condition (round 1/6 x 2/6 person x rotate/thin/thick)

```{r}
human_summary |> select(-source) |> rename(Human=correct) |> left_join(model_summary |> select(-source) |> rename(Model=correct)) |> ggplot(aes(x=Human, y=Model, color=tangram))+geom_point()+geom_smooth(aes(group=1),method="lm")+
  coord_equal(xlim=c(0,1), ylim=c(0,1))+
  geom_abline()
```

This is slightly unfair in some ways since they might be seeing different subsets. 


Model sees on a *per utterance* basis, humans see on a per transcript basis. It may in future make sense to show the model something more like what the people see if comparison is what we care about. 

## Taking only the first utterance

Assume first utterance is most contentful, and later ones may be more addressing questions or adding details. 

```{r}
model_first_utt <- d_subset |> group_by(gameId, trialNum) |> mutate(blah=row_number()) |> filter(blah==1) |> ungroup() 

model_summary_first <- model_first_utt|> 
  mutate(round = str_c("round_",as.character(repNum + 1))) |>
  mutate(correct = tangram == prediction) |>
  group_by(tangram, condition, round) |>
  summarize(correct = mean(correct)) |> 
  mutate(source="model")


both <- human_summary |> bind_rows(model_summary_first) 

label <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

foo <- tibble(tangram=label,labels) |> left_join(both) |> 
  group_by(tangram, labels) |> 
  summarize(acc=mean(correct)) |> 
  arrange(acc)

ggplot(both, aes(x=reorder(tangram, correct), y=correct, group=source, color=source))+geom_point(position=position_dodge(width=.3))+
    scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text = element_markdown(color = "black", size = 11))
```

## Taking only singleton utterances

This has less data especially in some conditions, but is the most comparable. 

```{r}

model_solo_utt <- d_subset |> group_by(gameId, trialNum) |> summarize(n=n()) |> filter(n==1) |> inner_join(d_subset) 

human_acc <- human |> group_by(gameId, round, condition, tangram=correct_tangram) |> summarize(human_correct=mean(correct))

joined_first <- model_first_utt |>   mutate(round = str_c("round_",as.character(repNum + 1))) |>
inner_join(human_acc) 

joined_solo <- model_solo_utt |>   mutate(round = str_c("round_",as.character(repNum + 1))) |>
inner_join(human_acc) 
```

```{r}

joined_solo |> group_by(condition, round) |> tally()

long_solo <- joined_solo |> group_by(tangram, condition,round) |> summarize(human=mean(human_correct), model=mean(prediction==label)) |> pivot_longer(human:model, names_to="source", values_to="acc") 

foo <- tibble(tangram=label,labels) |> left_join(long_solo) |> 
  group_by(tangram, labels) |> 
  summarize(acc=mean(acc)) |> 
  arrange(acc)

ggplot(long_solo, aes(x=reorder(tangram, acc), y=acc, group=source, color=source))+geom_point(position=position_dodge(width=.3))+
    scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text = element_markdown(color = "black", size = 11))
```

